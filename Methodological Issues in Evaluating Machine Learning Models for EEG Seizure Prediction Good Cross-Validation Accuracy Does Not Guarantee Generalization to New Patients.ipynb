{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa46a43f",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne_features.feature_extraction import FeatureExtractor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Different classifiers\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Hyperparameters tuning\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137c651",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451027b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_signal(eeg_signal, common_channels, sampling_rate, new_sampling_rate):\n",
    "    '''\n",
    "    Preprocess EEG signals by the MNE package.\n",
    "    \n",
    "    Parameters include EEG signal, a list of common scalp channel names\n",
    "    , sampling rate of the original signal, and new targeted sampling rate, respectively.\n",
    "    '''\n",
    "    \n",
    "    # Import data into the MNE package\n",
    "    eeg_signal = (1e-6) * (eeg_signal) # Convert data from microvolts to volts\n",
    "    ch_types = ['eeg' for _ in common_channels]\n",
    "    info = mne.create_info(common_channels, sampling_rate, ch_types = ch_types)\n",
    "    eeg_signal_mne = mne.io.RawArray(eeg_signal.T, info) # transpose and import the eeg signal into the MNE package\n",
    "    \n",
    "    # Apply filtering procedures and downsampling\n",
    "    eeg_signal_mne.notch_filter(np.asarray([50,100], dtype=float)) # Mitigate powerline interference and retain high freqs\n",
    "    eeg_signal_mne.filter(1, 125, fir_design = 'firwin') # Band pass filter on 1 and 125 Hz\n",
    "    eeg_signal_mne.resample(new_sampling_rate, npad = \"auto\")  # Resampling to the new_sampling_rate (Hz)\n",
    "    processed_eeg_signal = (eeg_signal_mne.to_data_frame()).drop(labels = \"time\", axis = 1)\n",
    "    \n",
    "    eeg_signal = np.asarray(processed_eeg_signal).T\n",
    "    \n",
    "    return eeg_signal\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(eeg_signal, window_size, window_step, new_sampling_rate):   \n",
    "    '''\n",
    "    Extract features from EEG channels by the MNE subpackage (mne_features).\n",
    "    \n",
    "    Parameters include processed EEG signal, the size of the window for feature extraction (5 seconds in this paper)\n",
    "    , the size of the window step (5 seconds in this study to apply non-overlapping)\n",
    "    , and new targeted sampling rate, respectively.\n",
    "    '''\n",
    "    \n",
    "    dataset = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(0, int(eeg_signal.shape[1])-window_size, window_step):\n",
    "        dataset_array = eeg_signal[: , j : j + window_size] # Select \"window_size\" from all channels\n",
    "        dataset_array = np.expand_dims(dataset_array, axis = 1)\n",
    "        dataset_array = dataset_array.reshape(1,1,window_size * int(eeg_signal.shape[0]))\n",
    "\n",
    "        # Extract 53 features (some APIs return more than one feature)\n",
    "        fe = FeatureExtractor(sfreq = new_sampling_rate\n",
    "                              , selected_funcs = ['mean', 'variance', 'std','ptp_amp', 'skewness'\n",
    "                                                  , 'kurtosis' ,'rms', 'quantile', 'decorr_time'\n",
    "                                                  ,'pow_freq_bands', 'hjorth_mobility_spect'\n",
    "                                                  , 'hjorth_complexity_spect','hjorth_mobility'\n",
    "                                                  , 'hjorth_complexity', 'higuchi_fd','katz_fd'\n",
    "                                                  , 'zero_crossings', 'line_length','spect_slope'\n",
    "                                                  , 'spect_entropy', 'energy_freq_bands', 'spect_edge_freq'\n",
    "                                                  ,'wavelet_coef_energy', 'teager_kaiser_energy'\n",
    "                                                  , 'max_cross_corr', 'phase_lock_val', 'nonlin_interdep'])\n",
    "        # Extra APIs: 'hurst_exp','app_entropy', 'samp_entropy', 'svd_entropy','svd_fisher_info', 'time_corr', 'spect_corr'\n",
    "            \n",
    "        dataset_array = fe.fit_transform(dataset_array)\n",
    "        dataset.extend(dataset_array)\n",
    "\n",
    "        # Assign labels\n",
    "        chunk_size = 30 * 60 * new_sampling_rate # 30 minutes\n",
    "        if j < chunk_size: # Preictal\n",
    "            labels.append(0) \n",
    "        elif j >= chunk_size * 8: # Interictal\n",
    "            labels.append(1) \n",
    "\n",
    "    dataset = np.asarray(dataset)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a8406",
   "metadata": {},
   "source": [
    "# Fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84574c9e",
   "metadata": {},
   "source": [
    "## 1) Randomized Cross-Validation validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, random_state=1, shuffle=True) # 5 folds cross-validation\n",
    "sc = StandardScaler() # Feature scaling\n",
    "\n",
    "# Select classifier\n",
    "classifier = XGBClassifier(eval_metric='mlogloss', use_label_encoder =False) # XGBoost\n",
    "#classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0) # Random Forest\n",
    "#classifier = GaussianNB() # Naive Bayes\n",
    "#classifier = LogisticRegression(max_iter=1000, random_state = 0) # Logistic Regression\n",
    "#classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # K-Nearest Neighbors (K-NN)\n",
    "#classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0) # Decision Tree\n",
    "#classifier = SVC(kernel = 'rbf', random_state = 0) # Kernel SVM\n",
    "#classifier = SVC(kernel = 'linear', random_state = 0) # Support Vector Machine (SVM)\n",
    "\n",
    "cm = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "\n",
    "for i, j in cv.split(dataset):\n",
    "    X_train, y_train, X_test, y_test = dataset[i], labels[i], dataset[j], labels[j]\n",
    "        \n",
    "    # Dataset balancing (apply customized balancing function \"balance_dataset\")\n",
    "    X_train, y_train = balance_dataset(X_train, y_train)\n",
    "\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    cm += confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity.append(tp / (tp+fn)) \n",
    "    specificity.append(tn / (tn+fp))\n",
    "\n",
    "# Plot confusion matrix\n",
    "time_sections = [\"preictal\", \"interictal\"]\n",
    "sns.heatmap(cm, cmap='Blues', annot=True, fmt='g', xticklabels = time_sections\n",
    "            , yticklabels = time_sections, annot_kws={\"size\": 14})\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title( \"K-fold Method\" )\n",
    "plt.show()\n",
    "\n",
    "# Print performance metrics values\n",
    "print(\"================================================\")\n",
    "print(\"accuracy: {:.2f} %\".format(np.asarray(accuracy).mean()*100))\n",
    "print(\"sensitivity: {:.2f} %\".format(np.asarray(sensitivity).mean()*100))\n",
    "print(\"specificity: {:.2f} %\".format(np.asarray(specificity).mean()*100))\n",
    "print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e9383",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters (XGBoost) by the Optuna framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eadae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    '''\n",
    "    Find the best hyperparameters.\n",
    "\n",
    "    '''\n",
    "        \n",
    "    cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    sc = StandardScaler() # Feature scaling\n",
    "    accuracies = []\n",
    "\n",
    "    for i, j in cv.split(dataset): \n",
    "        X_train, y_train, X_test, y_test = dataset[i], labels[i], dataset[j], labels[j]\n",
    "\n",
    "        # Dataset balancing (apply customized balancing function \"balance_dataset\")\n",
    "        X_train, y_train = balance_dataset(X_train, y_train)\n",
    "    \n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        \n",
    "        param = {\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"verbosity\": 0,\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            # use exact for small dataset.\n",
    "            \"tree_method\": \"exact\",\n",
    "            # defines booster, gblinear for linear functions.\n",
    "            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "            # L2 regularization weight.\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            # L1 regularization weight.\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            # sampling ratio for training data.\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "            # sampling according to each tree.\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        }\n",
    "\n",
    "        if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            # maximum depth of the tree, signifies complexity of the tree.\n",
    "            param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "            # minimum child weight, larger the term more conservative the tree.\n",
    "            param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "            # defines how selective algorithm is.\n",
    "            param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "        if param[\"booster\"] == \"dart\":\n",
    "            param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        \n",
    "        \n",
    "        #pruning_callback = optuna.integration.XGBoostPruningCallback(trial, observation_key=\"validation_-auc\")\n",
    "        #classifier = XGBClassifier(**param, fit_params={'callbacks': [pruning_callback]})\n",
    "        \n",
    "        classifier = XGBClassifier(**param)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "    return np.asarray(accuracies).mean()\n",
    "        \n",
    "        \n",
    "        \n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"================================================\")\n",
    "trial = study.best_trial\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best Accuracy: {:.2f} %\".format(trial.value*100))\n",
    "print(\"Best Hyperparameters: {}\".format(trial.params))\n",
    "print(\"================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = study.trials_dataframe()\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d97a1",
   "metadata": {},
   "source": [
    "## 2) Left One patient Out validation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba89c8",
   "metadata": {},
   "source": [
    "### Import one patient's data into the test set and the rest into the training set instead of defining \"cv\" and applying \"KFold\". Everything will then continue as before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unipd_mne_features",
   "language": "python",
   "name": "unipd_mne_features"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
